{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Create virtual enviroment to install sklearn development version 0.21.2 as there are memory \n",
    "# issues with sklearn current version 0.21.1\n",
    "conda activate myenvsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "class HQC(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"The Helstrom Quantum Centroid (HQC) classifier is a quantum-inspired supervised \n",
    "    classification approach for data with binary classes (ie. data with 2 classes only).\n",
    "                         \n",
    "    Parameters\n",
    "    ----------\n",
    "    rescale : int or float, default = 1\n",
    "        The dataset rescaling factor. A parameter used for rescaling the dataset. \n",
    "    n_copies : int, default = 1\n",
    "        The number of copies to take for each quantum density. This is equivalent to taking \n",
    "        the n-fold Kronecker tensor product for each quantum density.  \n",
    "    n_jobs : int, default = None\n",
    "        The number of CPU cores used when parallelizing. If -1 all CPUs are used. If 1 is given, \n",
    "        no parallel computing code is used at all. For n_jobs below -1, (n_cpus + 1 + n_jobs) \n",
    "        are used. Thus for n_jobs = -2, all CPUs but one are used. None is a marker for ‘unset’ \n",
    "        that will be interpreted as n_jobs = 1.\n",
    "    n_splits : int, default = 1\n",
    "        The number of subset splits performed on the input dataset row-wise and on the number \n",
    "        of eigenvalues/eigenvectors of the Quantum Helstrom observable for optimal speed \n",
    "        performance. For optimal speed, recommend using n_splits = int(numpy.ceil(Number of \n",
    "        CPU cores used/2)). If memory blow-out occurs, reduce n_splits.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : ndarray, shape (2,)\n",
    "        Sorted binary classes.\n",
    "    centroid_ : ndarray, shape (2, n_features + 1, n_features + 1)\n",
    "        Quantum Centroids for class with index 0 and 1 respectively.\n",
    "    q_Hels_obs_ : ndarray, shape (n_features + 1, n_features + 1)\n",
    "        Quantum Helstrom observable.\n",
    "    proj_sum_ : tuple, shape (2, n_features + 1, n_features + 1)\n",
    "        Sum of the projectors of the Quantum Helstrom observable's eigenvectors, which has\n",
    "        corresponding positive and negative eigenvalues respectively.\n",
    "    Hels_bound_ : float\n",
    "        Helstrom bound is the upper bound of the probability that one can correctly \n",
    "        discriminate whether a quantum density is of which of the two binary quantum density \n",
    "        pattern.          \n",
    "    \"\"\"\n",
    "    # Added binary_only tag as required by sklearn check_estimator\n",
    "    def _more_tags(self):\n",
    "        return {'binary_only': True}\n",
    "    \n",
    "    \n",
    "    # Initialize model hyperparameters\n",
    "    def __init__(self, rescale=1, n_copies=1, n_jobs=None, n_splits=1):\n",
    "        self.rescale = rescale\n",
    "        self.n_copies = n_copies\n",
    "        self.n_jobs = n_jobs\n",
    "        self.n_splits = n_splits\n",
    "        \n",
    "    \n",
    "    # Function for fit\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Perform HQC classification with the inverse of the standard stereographic \n",
    "        projection encoding, with the option to rescale the dataset prior to encoding.\n",
    "                \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The training input samples. An array of int or float.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The training input binary target values. An array of str, int or float.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        # Ensure target y is of non-regression type  \n",
    "        # Added as required by sklearn check_estimator\n",
    "        check_classification_targets(y)\n",
    "    \n",
    "        # Store binary classes and encode y into binary class indexes 0 and 1\n",
    "        self.classes_, y_class_index = np.unique(y, return_inverse = True)\n",
    "        \n",
    "        # Cast X to float to ensure all following calculations below are done in float  \n",
    "        # rather than integer\n",
    "        X = X.astype(float)\n",
    "        \n",
    "        # Rescale X\n",
    "        X = self.rescale*X\n",
    "        \n",
    "        # Calculate sum of squares of each row (sample) in X\n",
    "        X_sq_sum = (X**2).sum(axis = 1)\n",
    "        \n",
    "        # Number of rows in X\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Number of columns in X\n",
    "        n = X.shape[1]\n",
    "        \n",
    "        # Function to calculate X'    \n",
    "        def X_prime_func(i):\n",
    "            return (1 / (X_sq_sum[i] + 1)) \\\n",
    "                   *(np.concatenate((2*X, (X_sq_sum - 1).reshape(-1, 1)), axis = 1)[i, :])\n",
    "        \n",
    "        # Calculate X'\n",
    "        X_prime = np.array(Parallel(n_jobs = self.n_jobs) \\\n",
    "                          (delayed(X_prime_func)(i) for i in range(m)))\n",
    "        \n",
    "        # Function to calculate terms in the Quantum Centroids and quantum Helstrom \n",
    "        # observable for each class\n",
    "        def centroid_terms_func(i):\n",
    "            # Determine rows (samples) in X' belonging to either class\n",
    "            X_prime_class = X_prime[y_class_index == i]\n",
    "            \n",
    "            # Number of rows (samples) in X' belonging to either class\n",
    "            M_class = X_prime_class.shape[0]\n",
    "            \n",
    "            # Split X' belonging to either class into n_splits subsets, row-wise\n",
    "            X_prime_class_split = np.array_split(X_prime_class, \n",
    "                                                 indices_or_sections = self.n_splits, \n",
    "                                                 axis = 0)\n",
    "            \n",
    "            # Function to calculate terms in the Quantum Centroids and quantum Helstrom\n",
    "            # observable for each class, per subset split\n",
    "            def X_prime_class_split_func(j):\n",
    "                # Counter for j-th split X' belonging to either class\n",
    "                X_prime_class_split_jth = X_prime_class_split[j]\n",
    "                \n",
    "                # Number of rows (samples) in j-th split X' belonging to either class\n",
    "                M_class_split = X_prime_class_split_jth.shape[0]\n",
    "            \n",
    "                # Number of rows/columns in density matrix\n",
    "                density_nrow_ncol = (n + 1)**self.n_copies\n",
    "            \n",
    "                # Initialize arrays density_sum and centroid\n",
    "                density_sum = np.zeros((density_nrow_ncol, density_nrow_ncol))\n",
    "                centroid = density_sum\n",
    "                for k in range(M_class_split):\n",
    "                    # Encode into quantum densities using the inverse of the standard \n",
    "                    # stereographic projection encoding method\n",
    "                    X_prime_class_split_each_row = X_prime_class_split_jth[k, :]\n",
    "                    density_each_row = np.dot(X_prime_class_split_each_row.reshape(-1, 1),\n",
    "                                              X_prime_class_split_each_row.reshape(1, -1))\n",
    "                \n",
    "                    # Calculate n-fold Kronecker tensor product\n",
    "                    if self.n_copies == 1:\n",
    "                        density_each_row = density_each_row\n",
    "                    else:\n",
    "                        density_each_row_copy = density_each_row\n",
    "                        for u in range(self.n_copies - 1):\n",
    "                            density_each_row = np.kron(density_each_row, density_each_row_copy)\n",
    "                \n",
    "                    # Calculate sum of quantum densities belonging to either class\n",
    "                    density_sum = density_sum + density_each_row\n",
    "                \n",
    "                    # Calculate Quantum Centroid\n",
    "                    # Added ZeroDivisionError as required by sklearn check_estimator\n",
    "                    try:\n",
    "                        centroid = (1 / M_class)*density_sum\n",
    "                    except ZeroDivisionError:\n",
    "                        centroid = 0\n",
    "                return M_class_split, centroid, (1 / m)*density_sum        \n",
    "            return np.sum(Parallel(n_jobs = self.n_jobs) \\\n",
    "                         (delayed(X_prime_class_split_func)(j) for j in range(self.n_splits)), axis = 0)\n",
    "            \n",
    "        # Calculate Quantum Centroids and terms in the quantum Helstrom observable\n",
    "        centroid_terms = np.array(Parallel(n_jobs = self.n_jobs) \\\n",
    "                                          (delayed(centroid_terms_func)(i) for i in range(2)))\n",
    "        \n",
    "        # Determine Quantum Centroids\n",
    "        self.centroid_ = centroid_terms[:, 1]\n",
    "           \n",
    "        # Calculate quantum Helstrom observable\n",
    "        self.q_Hels_obs_ = centroid_terms[0, 2] - centroid_terms[1, 2]     \n",
    "        \n",
    "        # Calculate eigenvalues w and eigenvectors v of the quantum Helstrom observable\n",
    "        w, v = np.linalg.eigh(self.q_Hels_obs_)\n",
    "        \n",
    "        # Length of w\n",
    "        len_w = len(w)\n",
    "        \n",
    "        # Initialize array eigval_class\n",
    "        eigval_class = np.empty_like(w)\n",
    "        for i in range(len_w):\n",
    "            # Create an array of 0s and 1s to indicate positive and negative eigenvalues\n",
    "            # respectively\n",
    "            if w[i] > 0:\n",
    "                eigval_class[i] = 0\n",
    "            else:\n",
    "                eigval_class[i] = 1\n",
    "        \n",
    "        # Transpose matrix v containing eigenvectors to row-wise\n",
    "        eigvec = v.T\n",
    "        \n",
    "        # Function to calculate sum of the projectors corresponding to positive and negative\n",
    "        # eigenvalues respectively\n",
    "        def sum_proj_func(i):\n",
    "            # Determine eigenvectors belonging to positive and negative eigenvalues respectively\n",
    "            eigvec_class = eigvec[eigval_class == i]\n",
    "            \n",
    "            # Split eigenvectors into n_splits subsets\n",
    "            eigvec_class_split = np.array_split(eigvec_class, \n",
    "                                                indices_or_sections = self.n_splits, \n",
    "                                                axis = 0)\n",
    "            \n",
    "            # Function to calculate sum of the projectors corresponding to positive and negative\n",
    "            # eigenvalues respectively, per subset split\n",
    "            def eigvec_class_split_func(j):\n",
    "                # Initialize array proj_sum_split\n",
    "                proj_sum_split = np.zeros_like(self.q_Hels_obs_)\n",
    "                for k in eigvec_class_split[j]:\n",
    "                    # Calculate sum of the projectors corresponding to positive and negative\n",
    "                    # eigenvalues respectively, per subset split\n",
    "                    proj_sum_split = proj_sum_split + np.dot(k.reshape(-1, 1), k.reshape(1, -1))\n",
    "                return proj_sum_split        \n",
    "            return np.sum(Parallel(n_jobs = self.n_jobs) \\\n",
    "                         (delayed(eigvec_class_split_func)(j) for j in range(self.n_splits)), axis = 0)\n",
    "        \n",
    "        # Calculate sum of the projectors corresponding to positive and negative eigenvalues\n",
    "        # respectively\n",
    "        self.proj_sum_ = Parallel(n_jobs = self.n_jobs) \\\n",
    "                         (delayed(sum_proj_func)(i) for i in range(2))    \n",
    "                       \n",
    "        # Calculate Helstrom bound\n",
    "        self.Hels_bound_ = (centroid_terms[0, 0] / m)*np.einsum('ij,ji->', self.centroid_[0], \n",
    "                                                                self.proj_sum_[0]) \\\n",
    "                           + (centroid_terms[1, 0] / m)*np.einsum('ij,ji->', self.centroid_[1], \n",
    "                                                                  self.proj_sum_[1])\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    # Function for predict_proba\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Performs HQC classification on X and returns the trace of the dot product of the densities \n",
    "        and the sum of the projectors with corresponding positive and negative eigenvalues respectively.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples. An array of int or float.       \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        trace_matrix : array-like, shape (n_samples, 2)\n",
    "            Column index 0 corresponds to the trace of the dot product of the densities and the sum  \n",
    "            of projectors with positive eigenvalues. Column index 1 corresponds to the trace of the  \n",
    "            dot product of the densities and the sum of projectors with negative eigenvalues. An array \n",
    "            of float.\n",
    "        \"\"\"\n",
    "        # Check if fit had been called\n",
    "        check_is_fitted(self, ['proj_sum_'])\n",
    "\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # Cast X to float to ensure all following calculations below are done in float \n",
    "        # rather than integer\n",
    "        X = X.astype(float)        \n",
    "        \n",
    "        # Rescale X\n",
    "        X = self.rescale*X        \n",
    "        \n",
    "        # Calculate sum of squares of each row (sample) in X\n",
    "        X_sq_sum = (X**2).sum(axis = 1)\n",
    "        \n",
    "        # Number of rows in X\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Number of columns in X\n",
    "        n = X.shape[1]\n",
    "\n",
    "        # Function to calculate X'    \n",
    "        def X_prime_func(i):\n",
    "            return (1 / (X_sq_sum[i] + 1)) \\\n",
    "                   *(np.concatenate((2*X, (X_sq_sum - 1).reshape(-1, 1)), axis=1)[i, :])\n",
    "        \n",
    "        # Calculate X'\n",
    "        X_prime = np.array(Parallel(n_jobs = self.n_jobs) \\\n",
    "                          (delayed(X_prime_func)(i) for i in range(m)))        \n",
    "        \n",
    "        # Function to calculate trace values for each class\n",
    "        def trace_func(i):\n",
    "            # Split X' into n_splits subsets, row-wise\n",
    "            X_prime_split = np.array_split(X_prime, \n",
    "                                           indices_or_sections = self.n_splits, \n",
    "                                           axis = 0)\n",
    "            \n",
    "            # Function to calculate trace values for each class, per subset split\n",
    "            def trace_split_func(j):\n",
    "                # Counter for j-th split X'\n",
    "                X_prime_split_jth = X_prime_split[j]\n",
    "                \n",
    "                # Number of rows (samples) in j-th split X'\n",
    "                X_prime_split_m = X_prime_split_jth.shape[0]\n",
    "                \n",
    "                # Initialize array trace_class_split\n",
    "                trace_class_split = np.empty(X_prime_split_m)\n",
    "                for k in range(X_prime_split_m):\n",
    "                    # Encode into quantum densities using the inverse of the standard stereographic\n",
    "                    # projection encoding method\n",
    "                    X_prime_split_each_row = X_prime_split_jth[k, :]\n",
    "                    density_each_row = np.dot(X_prime_split_each_row.reshape(-1, 1), \n",
    "                                              X_prime_split_each_row.reshape(1, -1))\n",
    "                \n",
    "                    # Calculate n-fold Kronecker tensor product\n",
    "                    if self.n_copies == 1:     \n",
    "                        density_each_row = density_each_row\n",
    "                    else:\n",
    "                        density_each_row_copy = density_each_row\n",
    "                        for u in range(self.n_copies - 1):\n",
    "                            density_each_row = np.kron(density_each_row, density_each_row_copy)\n",
    "                        \n",
    "                    # Calculate trace of the dot product of density of each row and sum of projectors \n",
    "                    # with corresponding positive and negative eigenvalues respectively    \n",
    "                    trace_class_split[k] = np.einsum('ij,ji->', density_each_row, self.proj_sum_[i])\n",
    "                return trace_class_split\n",
    "            \n",
    "            # Calculate trace values for each class, per subset split\n",
    "            trace_class = Parallel(n_jobs = self.n_jobs) \\\n",
    "                          (delayed(trace_split_func)(j) for j in range(self.n_splits))\n",
    "            return np.concatenate(trace_class, axis = 0)\n",
    "            \n",
    "        # Calculate trace values for each class\n",
    "        trace_matrix = np.transpose(Parallel(n_jobs = self.n_jobs) \\\n",
    "                                   (delayed(trace_func)(i) for i in range(2)))\n",
    "        return trace_matrix\n",
    "        \n",
    "    \n",
    "    # Function for predict\n",
    "    def predict(self, X):\n",
    "        \"\"\"Performs HQC classification on X and returns the binary classes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples. An array of int or float.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self.classes_[predict_trace_index] : array-like, shape (n_samples,)\n",
    "            The predicted binary classes. An array of str, int or float.\n",
    "        \"\"\"\n",
    "        # Determine column index with the higher trace value in trace_matrix\n",
    "        # If both columns have the same trace value, returns column index 0\n",
    "        predict_trace_index = np.argmax(self.predict_proba(X), axis = 1)\n",
    "        # Returns the predicted binary classes\n",
    "        return self.classes_[predict_trace_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appendicitis dataset (7 features, 106 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('appendicitis.tsv',delimiter='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values\n",
    "\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7520661157024794, 0.8772542482734037)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score and Helstrom bound values for various rescale and n_copies values\n",
    "model = HQC(rescale=0.5, n_copies=3, n_jobs=-1, n_splits=2).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted'), model.Hels_bound_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490 ms ± 77.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=1\n",
    "%timeit HQC(rescale=0.5, n_copies=1, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 ms ± 52.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=2\n",
    "%timeit HQC(rescale=0.5, n_copies=2, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.14 s ± 84 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=3\n",
    "%timeit HQC(rescale=0.5, n_copies=3, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10min 50s ± 32.4 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=4\n",
    "%timeit HQC(rescale=0.5, n_copies=4, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 8.00 GiB for an array with shape (32768, 32768) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 608, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"<ipython-input-2-e235a42292fd>\", line 143, in centroid_and_q_Hels_obs_func\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1017, in __call__\n    self.retrieve()\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 909, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 657, in get\n    raise self._value\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 608, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\HP\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 256, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"<ipython-input-2-e235a42292fd>\", line 116, in X_prime_class_split_func\nMemoryError: Unable to allocate 8.00 GiB for an array with shape (32768, 32768) and data type float64\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2c214b313b28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'timeit'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'HQC(rescale=0.5, n_copies=5, n_jobs=-1).fit(X_train, y_train)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2315\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2317\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2318\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-61>\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1158\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1160\u001b[1;33m                 \u001b[0mtime_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1161\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[1;34m(self, number)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mtiming\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[1;34m(_it, _timer)\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-e235a42292fd>\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[1;31m# Calculate Quantum Centroids and terms in the quantum Helstrom observable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         centroid_and_q_Hels_obs = np.array(Parallel(n_jobs = self.n_jobs) \\\n\u001b[1;32m--> 147\u001b[1;33m                                           (delayed(centroid_and_q_Hels_obs_func)(i) for i in range(2)))\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;31m# Determine Quantum Centroids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    433\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 8.00 GiB for an array with shape (32768, 32768) and data type float64"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=5. Memory blow-out\n",
    "%timeit HQC(rescale=0.5, n_copies=5, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 119.21 MiB, increment: 0.08 MiB\n"
     ]
    }
   ],
   "source": [
    "# Memory required for n_copies=1\n",
    "%memit HQC(rescale=0.5, n_copies=1, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 120.38 MiB, increment: 0.03 MiB\n"
     ]
    }
   ],
   "source": [
    "# Memory required for n_copies=2\n",
    "%memit HQC(rescale=0.5, n_copies=2, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 136.98 MiB, increment: 16.61 MiB\n"
     ]
    }
   ],
   "source": [
    "# Memory required for n_copies=3\n",
    "%memit HQC(rescale=0.5, n_copies=3, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# banana dataset (2 features, 5300 rows)\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('banana.tsv', sep='\\t')\n",
    "X = df.drop('target', axis=1).values\n",
    "y = df['target'].values\n",
    "\n",
    "from sklearn import model_selection\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.858978398722441, 0.7732939055876817)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check F1 score and Helstrom bound values for various rescale and n_copies values\n",
    "model = HQC(rescale=0.5, n_copies=4, n_jobs=-1, n_splits=2).fit(X_train, y_train)\n",
    "y_hat = model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "metrics.f1_score(y_test, y_hat, average='weighted'), model.Hels_bound_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21 s ± 91.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=1\n",
    "%timeit HQC(rescale=0.5, n_copies=1, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5 s ± 45.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=2\n",
    "%timeit HQC(rescale=0.5, n_copies=2, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.86 s ± 83.6 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=3\n",
    "%timeit HQC(rescale=0.5, n_copies=3, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.77 s ± 135 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=4\n",
    "%timeit HQC(rescale=0.5, n_copies=4, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.18 s ± 131 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=5\n",
    "%timeit HQC(rescale=0.5, n_copies=5, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.1 s ± 944 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=6\n",
    "%timeit HQC(rescale=0.5, n_copies=6, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9min 23s ± 12.9 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "# Time required for n_copies=7\n",
    "%timeit HQC(rescale=0.5, n_copies=7, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The memory_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext memory_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 109.55 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# Memory required for n_copies=1\n",
    "%memit HQC(rescale=0.5, n_copies=1, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 109.30 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# Memory required for n_copies=2\n",
    "%memit HQC(rescale=0.5, n_copies=2, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 109.30 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# Memory required for n_copies=3\n",
    "%memit HQC(rescale=0.5, n_copies=3, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 110.30 MiB, increment: 1.00 MiB\n"
     ]
    }
   ],
   "source": [
    "# Memory required for n_copies=4\n",
    "%memit HQC(rescale=0.5, n_copies=4, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 114.02 MiB, increment: 3.71 MiB\n"
     ]
    }
   ],
   "source": [
    "# Memory required for n_copies=5\n",
    "%memit HQC(rescale=0.5, n_copies=5, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 127.69 MiB, increment: 42.93 MiB\n"
     ]
    }
   ],
   "source": [
    "# Memory required for n_copies=6\n",
    "%memit HQC(rescale=0.5, n_copies=6, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 493.77 MiB, increment: 382.32 MiB\n"
     ]
    }
   ],
   "source": [
    "# Memory required for n_copies=7\n",
    "%memit HQC(rescale=0.5, n_copies=7, n_jobs=-1, n_splits=2).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
