{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HQC - GPU 3.ipynb","provenance":[],"authorship_tag":"ABX9TyMIRnoHoWA3NHLZYZ3qkHHk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"VCUm0OcMFLX_","colab_type":"code","colab":{}},"source":["# I have implemented the code below in such a way that you would only need to input X and y as numpy arrays and the\n","# output y_hat would also be a numpy array (rather than PyTorch tensors). This would make it easier to use the package\n","# below with minimal knowledge of PyTorch tensors.\n","\n","# Take note of the parameter n_splits, where the implementation of n_splits now is different to the one in the CPU case.\n","# Please read the description of n_splits below."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qN2LAzGNb5B0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597037590547,"user_tz":-600,"elapsed":5417,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}}},"source":["import numpy as np\n","from sklearn.base import BaseEstimator, ClassifierMixin\n","from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n","from sklearn.utils.multiclass import check_classification_targets\n","import torch\n","from torch.nn.functional import normalize\n","\n","class HQC(BaseEstimator, ClassifierMixin):\n","    \"\"\"The Helstrom Quantum Centroid (HQC) classifier is a quantum-inspired supervised \n","    classification approach for data with binary classes (ie. data with 2 classes only).\n","                         \n","    Parameters\n","    ----------\n","    rescale : int or float, default = 1\n","        The dataset rescaling factor. A parameter used for rescaling the dataset. \n","    encoding : str, default = 'amplit'\n","        The encoding method used to encode vectors into quantum densities. Possible values:\n","        'amplit', 'stereo'. 'amplit' means using the amplitude encoding method. 'stereo' means \n","        using the inverse of the standard stereographic projection encoding method. Default set \n","        to 'amplit'.\n","    n_copies : int, default = 1\n","        The number of copies to take for each quantum density. This is equivalent to taking \n","        the n-fold Kronecker tensor product for each quantum density.\n","    class_wgt : str, default = 'equi'\n","        The class weights assigned to the Quantum Helstrom observable terms. Possible values: \n","        'equi', 'weighted'. 'equi' means assigning equal weights of 1/2 (equiprobable) to the\n","        two classes in the Quantum Helstrom observable. 'weighted' means assigning weights equal \n","        to the proportion of the number of rows in each class to the two classes in the Quantum \n","        Helstrom observable. Default set to 'equi'.\n","    n_splits : int, default = 1\n","        The number of subset splits performed on the input dataset row-wise and on the number \n","        of eigenvalues/eigenvectors of the Quantum Helstrom observable for optimal speed \n","        performance. If 1 is given, no subset splits are used. For optimal speed, recommend \n","        using small values as close to 1 as possible. Value given cannot be more than the number \n","        of rows in the training or test datasets. If memory blow-out occurs, increase n_splits.\n","    \n","    Attributes\n","    ----------\n","    classes_ : tensor, size (2)\n","        Sorted binary classes. Stored in GPU.\n","    centroids_ : tensor, size (2, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n","        Quantum Centroids for class with index 0 and 1 respectively. Stored in GPU.\n","    hels_obs_ : tensor, size ((n_features + 1)**n_copies, (n_features + 1)**n_copies)\n","        Quantum Helstrom observable. Stored in GPU.\n","    proj_sums_ : tensor, size (2, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n","        Sum of the projectors of the Quantum Helstrom observable's eigenvectors, which has\n","        corresponding positive and negative eigenvalues respectively. Stored in GPU.\n","    hels_bound_ : float\n","        Helstrom bound is the upper bound of the probability that one can correctly \n","        discriminate whether a quantum density is of which of the two binary quantum density \n","        pattern. Stored in CPU.         \n","    \"\"\"\n","    # Added binary_only tag as required by sklearn check_estimator\n","    def _more_tags(self):\n","        return {'binary_only': True}\n","    \n","    \n","    # Initialize model hyperparameters\n","    def __init__(self, \n","                 rescale = 1,\n","                 encoding = 'amplit',\n","                 n_copies = 1,                   \n","                 class_wgt = 'equi', \n","                 n_splits = 1):\n","        self.rescale = rescale\n","        self.encoding = encoding\n","        self.n_copies = n_copies\n","        self.class_wgt = class_wgt\n","        self.n_splits = n_splits\n","        \n","    \n","    # Function for kronecker tensor product of PyTorch tensors, set as global function\n","    global kronecker\n","    def kronecker(A, B):\n","        return torch.einsum('nab,ncd->nacbd', A, B).view(A.size(0), \n","                                                         A.size(1)*B.size(1), \n","                                                         A.size(2)*B.size(2))\n","    \n","    \n","    # Function for fit\n","    def fit(self, X, y):\n","        \"\"\"Perform HQC classification with the inverse of the standard stereographic \n","        projection encoding, with the option to rescale the dataset prior to encoding.\n","                \n","        Parameters\n","        ----------\n","        X : array-like, shape (n_samples, n_features)\n","            The training input samples. An array of int or float.\n","        y : array-like, shape (n_samples,)\n","            The training input binary target values. An array of int or float only. No str.\n","            \n","        Returns\n","        -------\n","        self : object\n","            Returns self.\n","        \"\"\"\n","        # Check that arrays X and y have correct shape\n","        X, y = check_X_y(X, y)\n","        \n","        # Ensure target array y is of non-regression type  \n","        # Added as required by sklearn check_estimator\n","        check_classification_targets(y)\n","            \n","        # Cast array y into a tensor and send tensor y from CPU to GPU, store binary classes\n","        # and encode tensor y into binary class indeces 0 and 1\n","        self.classes_, y_class_index = torch.unique(torch.LongTensor(y).cuda(), \n","                                                    return_inverse = True)\n","        \n","        # Cast array X into a floating point tensor to ensure all following calculations below  \n","        # are done in float rather than integer, and send tensor X from CPU to GPU\n","        X = torch.DoubleTensor(X).cuda()\n","        \n","        # Rescale X\n","        X = self.rescale*X\n","        \n","        # Calculate sum of squares of each row (sample) in X\n","        X_sq_sum = (X**2).sum(dim = 1)\n","        \n","        # Number of rows in X\n","        m = X.shape[0]\n","        \n","        # Number of columns in X\n","        n = X.shape[1]\n","        \n","        # Calculate X' using amplitude or inverse of the standard stereographic projection \n","        # encoding method\n","        if self.encoding == 'amplit':\n","            X_prime = normalize(torch.cat([X, torch.ones(m, dtype=torch.float64) \\\n","                                           .reshape(-1, 1).cuda()], dim = 1), p = 2, dim = 1)\n","        elif self.encoding == 'stereo':\n","            X_prime = (1 / (X_sq_sum + 1)).reshape(-1, 1)*(torch.cat((2*X, (X_sq_sum - 1) \\\n","                                                                      .reshape(-1, 1)), dim = 1))\n","        else:\n","            raise ValueError('encoding should be \"amplit\" or \"stereo\"')\n","        \n","        # Number of columns in X', set as global variable\n","        global n_prime\n","        n_prime = n + 1\n","        \n","        # Function to calculate terms in the Quantum Centroids and quantum Helstrom \n","        # observable for each class, per subset split\n","        def centroids_terms_func(i):\n","            # Determine rows (samples) in X' belonging to either class\n","            X_prime_class = X_prime[y_class_index == i]\n","                                    \n","            # Split X' belonging to either class into n_splits subsets, row-wise\n","            # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n","            # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n","            X_prime_class_split_arr = np.array_split(X_prime_class.cpu().numpy(),\n","                                                     indices_or_sections = self.n_splits,\n","                                                     axis = 0)\n","            # Cast arrays back to tensors and send back from CPU to GPU\n","            X_prime_class_split = [torch.DoubleTensor(a).cuda() for a in X_prime_class_split_arr]\n","            \n","            # Function to calculate sum of quantum densities belonging to each class, \n","            # per subset split\n","            def X_prime_class_split_func(j):\n","                # Counter for j-th split of X'\n","                X_prime_class_split_jth = X_prime_class_split[j]\n","                \n","                # Number of rows (samples) in j-th split of X'\n","                m_class_split = X_prime_class_split_jth.shape[0]\n","                \n","                # Encode vectors into quantum densities\n","                density_chunk = torch.matmul(X_prime_class_split_jth.view(m_class_split, \n","                                                                          n_prime, 1),\n","                                             X_prime_class_split_jth.view(m_class_split, \n","                                                                          1, n_prime))\n","                \n","                # Calculate n-fold Kronecker tensor product\n","                if self.n_copies == 1:\n","                    density_chunk = density_chunk\n","                else:\n","                    density_chunk_copy = density_chunk\n","                    for b in range(self.n_copies - 1):\n","                        density_chunk = kronecker(density_chunk, density_chunk_copy)\n","                    \n","                # Calculate sum of quantum densities\n","                density_chunk_sum = density_chunk.sum(dim = 0)\n","                return density_chunk_sum\n","            \n","            # Number of rows/columns in density matrix, set as global variable\n","            global density_nrow_ncol\n","            density_nrow_ncol = n_prime**self.n_copies\n","            \n","            # Initialize array density_class_sum\n","            density_class_sum = torch.zeros([density_nrow_ncol, density_nrow_ncol], \n","                                            dtype = torch.float64).cuda()\n","            # Calculate sum of quantum densities belonging to either class\n","            for c in range(self.n_splits):\n","                density_class_sum = density_class_sum + X_prime_class_split_func(c)\n","            \n","            # Number of rows (samples) in X' belonging to either class\n","            m_class = X_prime_class.shape[0]\n","            \n","            # Function to calculate centroid belonging to either class\n","            def centroid():\n","                # Calculate Quantum Centroid belonging to either class\n","                # Added ZeroDivisionError as required by sklearn check_estimator\n","                try:\n","                    centroid = (1 / m_class)*density_class_sum\n","                except ZeroDivisionError:\n","                    centroid = 0 \n","                return centroid\n","            \n","            # Calculate centroid belonging to either class\n","            centroid_class = centroid()\n","            \n","            # Calculate terms in the quantum Helstrom observable belonging to either class\n","            if self.class_wgt == 'equi':\n","                hels_obs_terms = 0.5*centroid_class\n","            elif self.class_wgt == 'weighted':\n","                hels_obs_terms = (m_class / m)*centroid_class\n","            else:\n","                raise ValueError('class_wgt should be \"equi\" or \"weighted\"')\n","            return m_class, centroid_class, hels_obs_terms\n","        \n","        # Calculate Quantum Centroids and terms in the quantum Helstrom observable belonging \n","        # to either class\n","        centroids_terms = [centroids_terms_func(0), centroids_terms_func(1)] \n","                    \n","        # Determine Quantum Centroids\n","        self.centroids_ = torch.stack([centroids_terms[0][1], centroids_terms[1][1]], dim = 0)\n","                \n","        # Calculate quantum Helstrom observable\n","        self.hels_obs_ = centroids_terms[0][2] - centroids_terms[1][2] \n","                \n","        # Calculate eigenvalues w and eigenvectors v of the quantum Helstrom observable\n","        w, v = torch.symeig(self.hels_obs_, eigenvectors = True)\n","          \n","        # Length of w\n","        len_w = len(w)\n","        \n","        # Initialize array eigval_class\n","        eigval_class = torch.empty_like(w, dtype = torch.float64).cuda()\n","        for d in range(len_w):\n","            # Create an array of 0s and 1s to indicate positive and negative eigenvalues\n","            # respectively\n","            if w[d] > 0:\n","                eigval_class[d] = 0\n","            else:\n","                eigval_class[d] = 1\n","        \n","        # Transpose matrix v containing eigenvectors to row-wise\n","        eigvec = v.T\n","        \n","        # Function to calculate sum of the projectors corresponding to positive and negative\n","        # eigenvalues respectively\n","        def sum_proj_func(e):\n","            # Split eigenvectors belonging to positive or negative eigenvalues into n_splits subsets\n","            # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n","            # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n","            eigvec_class_split_arr = np.array_split(eigvec.cpu().numpy()[eigval_class.cpu() == e],\n","                                                    indices_or_sections = self.n_splits,\n","                                                    axis = 0)\n","            # Cast arrays back to tensors and send back from CPU to GPU\n","            eigvec_class_split = [torch.DoubleTensor(f).cuda() for f in eigvec_class_split_arr]             \n","            \n","            # Function to calculate sum of the projectors corresponding to positive and negative\n","            # eigenvalues respectively, per subset split\n","            def eigvec_class_split_func(g):\n","                # Counter for g-th split of eigvec\n","                eigvec_class_split_gth = eigvec_class_split[g]\n","                \n","                # Number of rows (samples) in g-th split of eigvec\n","                m_eigvec_class_split = eigvec_class_split_gth.shape[0]\n","                \n","                # Calculate projectors corresponding to positive and negative eigenvalues  \n","                # respectively, per subset split\n","                proj_split = torch.matmul(eigvec_class_split_gth.view(m_eigvec_class_split, \n","                                                                      density_nrow_ncol, 1),\n","                                          eigvec_class_split_gth.view(m_eigvec_class_split, \n","                                                                      1, density_nrow_ncol))\n","                \n","                # Calculate sum of projectors\n","                proj_split_sum = proj_split.sum(dim = 0)\n","                return proj_split_sum\n","            \n","            # Initialize array proj_class_sum\n","            proj_class_sum = torch.zeros([density_nrow_ncol, density_nrow_ncol], \n","                                         dtype = torch.float64).cuda()  \n","            # Calculate sum of the projectors corresponding to positive and negative eigenvalues \n","            # respectively\n","            for h in range(self.n_splits):\n","                proj_class_sum = proj_class_sum + eigvec_class_split_func(h)\n","            return proj_class_sum\n","        \n","        # Calculate sum of the projectors corresponding to positive and negative eigenvalues \n","        # respectively\n","        self.proj_sums_ = torch.stack([sum_proj_func(0), sum_proj_func(1)], dim = 0)        \n","                       \n","        # Calculate Helstrom bound\n","        self.hels_bound_ = (centroids_terms[0][0] / m)*torch.einsum('ij,ji->', self.centroids_[0], \n","                                                                   self.proj_sums_[0]).item() \\\n","                           + (centroids_terms[1][0] / m)*torch.einsum('ij,ji->', self.centroids_[1], \n","                                                                     self.proj_sums_[1]).item()\n","        return self\n","        \n","    \n","    # Function for predict_proba\n","    def predict_proba(self, X):\n","        \"\"\"Performs HQC classification on X and returns the trace of the dot product of the densities \n","        and the sum of the projectors with corresponding positive and negative eigenvalues respectively.\n","        \n","        Parameters\n","        ----------\n","        X : array-like, shape (n_samples, n_features)\n","            The input samples. An array of int or float.       \n","            \n","        Returns\n","        -------\n","        trace_matrix : tensor, size (n_samples, 2)\n","            Column index 0 corresponds to the trace of the dot product of the densities and the sum  \n","            of projectors with positive eigenvalues. Column index 1 corresponds to the trace of the  \n","            dot product of the densities and the sum of projectors with negative eigenvalues. A tensor \n","            of float. Stored in GPU.\n","        \"\"\"\n","        # Send tensor self.proj_sums_ from GPU to CPU and cast into an array\n","        self.proj_sums_arr_ = self.proj_sums_.cpu().numpy()\n","                \n","        # Check if fit had been called\n","        check_is_fitted(self, ['proj_sums_arr_'])\n","               \n","        # Input validation of array X\n","        X = check_array(X)\n","                 \n","        # Cast array X into a floating point tensor to ensure all following calculations below  \n","        # are done in float rather than integer, and send tensor X from CPU to GPU\n","        X = torch.DoubleTensor(X).cuda()\n","        \n","        # Rescale X\n","        X = self.rescale*X        \n","        \n","        # Calculate sum of squares of each row (sample) in X\n","        X_sq_sum = (X**2).sum(dim = 1)\n","        \n","        # Number of rows in X\n","        m = X.shape[0]\n","        \n","        # Number of columns in X\n","        n = X.shape[1]\n","\n","        # Calculate X' using amplitude or inverse of the standard stereographic projection \n","        # encoding method\n","        if self.encoding == 'amplit':\n","            X_prime = normalize(torch.cat([X, torch.ones(m, dtype=torch.float64) \\\n","                                           .reshape(-1, 1).cuda()], dim = 1), p = 2, dim = 1)\n","        elif self.encoding == 'stereo':\n","            X_prime = (1 / (X_sq_sum + 1)).reshape(-1, 1)*(torch.cat((2*X, (X_sq_sum - 1) \\\n","                                                                      .reshape(-1, 1)), dim = 1))\n","        else:\n","            raise ValueError('encoding should be \"amplit\" or \"stereo\"')\n","                       \n","        # Function to calculate trace values for each class\n","        def trace_func(i):\n","            # Split X' into n_splits subsets, row-wise\n","            # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n","            # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n","            X_prime_split_arr = np.array_split(X_prime.cpu().numpy(),\n","                                               indices_or_sections = self.n_splits,\n","                                               axis = 0)\n","            # Cast arrays back to tensors and send back from CPU to GPU\n","            X_prime_split = [torch.DoubleTensor(q).cuda() for q in X_prime_split_arr]\n","            \n","            # Function to calculate trace values for each class, per subset split\n","            def trace_split_func(j):\n","                # Counter for j-th split X'\n","                X_prime_split_jth = X_prime_split[j]\n","                \n","                # Number of rows (samples) in j-th split X'\n","                X_prime_split_m = X_prime_split_jth.shape[0]\n","                \n","                # Encode vectors into quantum densities\n","                density_chunk = torch.matmul(X_prime_split_jth.view(X_prime_split_m, n_prime, 1),\n","                                             X_prime_split_jth.view(X_prime_split_m, 1, n_prime))\n","                \n","                # Calculate n-fold Kronecker tensor product\n","                if self.n_copies == 1:\n","                    density_chunk = density_chunk\n","                else:\n","                    density_chunk_copy = density_chunk\n","                    for a in range(self.n_copies - 1):\n","                        density_chunk = kronecker(density_chunk, density_chunk_copy)\n","                        \n","                # Calculate trace of the dot product of density of each row and sum of projectors\n","                # with corresponding positive and negative eigenvalues respectively\n","                return torch.einsum('bij,ji->b', density_chunk, self.proj_sums_[i])\n","            \n","            # Initialize array trace_class\n","            trace_class = torch.empty([0], dtype = torch.float64).cuda()\n","            for b in range(self.n_splits):\n","                # Calculate trace values for each class, per subset split\n","                trace_class = torch.cat([trace_class, trace_split_func(b)], dim = 0)\n","            return trace_class\n","        \n","        # Calculate trace values for each class\n","        trace_matrix = torch.stack([trace_func(0), trace_func(1)], dim = 1)\n","        return trace_matrix\n","                \n","    \n","    # Function for predict\n","    def predict(self, X):\n","        \"\"\"Performs HQC classification on X and returns the binary classes.\n","        \n","        Parameters\n","        ----------\n","        X : array-like, shape (n_samples, n_features)\n","            The input samples. An array of int or float.\n","            \n","        Returns\n","        -------\n","        self.classes_[predict_trace_index] : array-like, shape (n_samples,)\n","            The predicted binary classes. An array of int or float only. No str. Store in CPU.\n","        \"\"\"\n","        # Determine column index with the higher trace value in trace_matrix\n","        # If both columns have the same trace value, returns column index 1, which is different \n","        # to np.argmax() which returns column index 0\n","        predict_trace_index = torch.argmax(self.predict_proba(X), axis = 1)\n","        # Returns the predicted binary classes\n","        return self.classes_[predict_trace_index].cpu().numpy()"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"bX45p7G1ou-9","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597037598340,"user_tz":-600,"elapsed":689,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}}},"source":["# appendicitis dataset (7 features, 106 rows)\n","import pandas as pd\n","\n","df = pd.read_csv('appendicitis.tsv',delimiter='\\t')\n","X = df.drop('target', axis=1).values\n","y = df['target'].values\n","\n","from sklearn import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"6lhrEuA8oyb1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597037996196,"user_tz":-600,"elapsed":1035,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"ccf359b4-afb2-4dc0-b60f-99aea98f460e"},"source":["# Check F1 score and Helstrom bound values for various rescale and n_copies values\n","model = HQC(rescale=0.5, n_copies=3, encoding='stereo', class_wgt='weighted', n_splits=1).fit(X_train, y_train)\n","y_hat = model.predict(X_test)\n","\n","from sklearn import metrics\n","metrics.f1_score(y_test, y_hat, average='weighted'), model.hels_bound_"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.7520661157024794, 0.8772542482734038)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"X_pmL9U92efw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597037637003,"user_tz":-600,"elapsed":3102,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"dcd579b6-2c7e-49c0-9823-705aa0486823"},"source":["# Time required for n_copies=1\n","%timeit HQC(rescale=0.5, n_copies=1, encoding='stereo', class_wgt='weighted', n_splits=1).fit(X_train, y_train)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["100 loops, best of 3: 5.59 ms per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7b8moYFR2eW2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597037676833,"user_tz":-600,"elapsed":6104,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"1ff6f80b-b133-42ff-e8cb-46400da76d13"},"source":["# Time required for n_copies=2\n","%timeit HQC(rescale=0.5, n_copies=2, encoding='stereo', class_wgt='weighted', n_splits=1).fit(X_train, y_train)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["100 loops, best of 3: 13.1 ms per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pxU5Ktyg2eIf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597037695050,"user_tz":-600,"elapsed":5976,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"846dba86-d3af-4586-8a1e-f5fadb9e86cd"},"source":["# Time required for n_copies=3\n","%timeit HQC(rescale=0.5, n_copies=3, encoding='stereo', class_wgt='weighted', n_splits=1).fit(X_train, y_train)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["10 loops, best of 3: 123 ms per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-mUUxolYel8C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597037815962,"user_tz":-600,"elapsed":52438,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"f5ac90c7-c0e3-4b3e-9012-358e44d5960d"},"source":["# Time required for n_copies=4\n","%timeit HQC(rescale=0.5, n_copies=4, encoding='stereo', class_wgt='weighted', n_splits=50).fit(X_train, y_train)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["1 loop, best of 3: 12.9 s per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sZ17YOmmfup5","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3q7jWEUy4GQU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597038269058,"user_tz":-600,"elapsed":638,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}}},"source":["# banana dataset (2 features, 5300 rows)\n","import pandas as pd\n","\n","df = pd.read_csv('banana.tsv', sep='\\t')\n","X = df.drop('target', axis=1).values\n","y = df['target'].values\n","\n","from sklearn import model_selection\n","X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=4)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"vPcbP4Y84JkW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597038273605,"user_tz":-600,"elapsed":674,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"db40d3af-22fb-453f-f826-d1690aa919dc"},"source":["# Check F1 score and Helstrom bound values for various rescale and n_copies values\n","model = HQC(rescale=0.5, n_copies=4, encoding='stereo', class_wgt='weighted', n_splits=1).fit(X_train, y_train)\n","y_hat = model.predict(X_test)\n","\n","from sklearn import metrics\n","metrics.f1_score(y_test, y_hat, average='weighted'), model.hels_bound_"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.858978398722441, 0.7732939055876822)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"xy_E4x1-4SUh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597038293671,"user_tz":-600,"elapsed":3582,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"738d8173-27b7-41c0-8647-d0a51c73313f"},"source":["# Time required for n_copies=1\n","%timeit HQC(rescale=0.5, n_copies=1, encoding='stereo', class_wgt='weighted', n_splits=1).fit(X_train, y_train)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["100 loops, best of 3: 7.04 ms per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yLipaKnH4Sf9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597038313096,"user_tz":-600,"elapsed":4014,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"72e019e1-7999-4b19-f292-500774d14cde"},"source":["# Time required for n_copies=2\n","%timeit HQC(rescale=0.5, n_copies=2, encoding='stereo', class_wgt='weighted', n_splits=1).fit(X_train, y_train)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["100 loops, best of 3: 7.87 ms per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NXpX2-mv4SjY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597038331262,"user_tz":-600,"elapsed":5215,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"ebc32585-39fb-42da-c641-5ab02f5c7de9"},"source":["# Time required for n_copies=3\n","%timeit HQC(rescale=0.5, n_copies=3, encoding='stereo', class_wgt='weighted', n_splits=1).fit(X_train, y_train)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["100 loops, best of 3: 10.8 ms per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Kx4RC2Xb4SnN","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597038345075,"user_tz":-600,"elapsed":1713,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"786fd0f0-40cf-4a69-84d4-22dfd8fce8e8"},"source":["# Time required for n_copies=4\n","%timeit HQC(rescale=0.5, n_copies=4, encoding='stereo', class_wgt='weighted', n_splits=1).fit(X_train, y_train)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["10 loops, best of 3: 25.4 ms per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CyZK8y5o4SsL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597038362615,"user_tz":-600,"elapsed":4952,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"96dbc5eb-9173-4cec-a048-872a99b88950"},"source":["# Time required for n_copies=5\n","%timeit HQC(rescale=0.5, n_copies=5, encoding='stereo', class_wgt='weighted', n_splits=1).fit(X_train, y_train)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["10 loops, best of 3: 98.2 ms per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TmcdJ5d95Ovj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597038679976,"user_tz":-600,"elapsed":3330,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"9784138d-69a3-4138-8c53-3c628d54a808"},"source":["# Time required for n_copies=6\n","%timeit HQC(rescale=0.5, n_copies=6, encoding='stereo', class_wgt='weighted', n_splits=2).fit(X_train, y_train)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["1 loop, best of 3: 648 ms per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ii-ivLVF5Ozw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597038723947,"user_tz":-600,"elapsed":28941,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"b74b95ca-b557-440a-b788-c7aa73d0c1c8"},"source":["# Time required for n_copies=7\n","%timeit HQC(rescale=0.5, n_copies=7, encoding='stereo', class_wgt='weighted', n_splits=18).fit(X_train, y_train)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["1 loop, best of 3: 7.02 s per loop\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"D32Y0bLE4Sqt","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hSgQRu0jdKo8","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597038512722,"user_tz":-600,"elapsed":2078,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}}},"source":["# Using scikit-learn's GridSearchCV\n","from sklearn.model_selection import GridSearchCV\n","import pandas as pd\n","\n","param_grid = {'rescale':[0.5, 1, 1.5], 'encoding':['amplit', 'stereo'], 'n_copies':[1, 2], 'class_wgt':['equi', 'weighted']}\n","models = GridSearchCV(HQC(), param_grid).fit(X_train, y_train)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXEI_XDDd3V2","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597038515971,"user_tz":-600,"elapsed":900,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"f09e8cb5-f6ca-4b16-adbb-f7b2f0707d09"},"source":["# Best F1 score\n","best_model = models.best_estimator_\n","y_hat = best_model.predict(X_test)\n","\n","from sklearn import metrics\n","metrics.f1_score(y_test, y_hat, average='weighted'), model.hels_bound_"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(0.7867484392554756, 0.7732939055876822)"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"5Ek5HAdGeZBl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597038521419,"user_tz":-600,"elapsed":648,"user":{"displayName":"Leo Chow","photoUrl":"","userId":"14333577283681602670"}},"outputId":"39b20b3c-78bf-44ab-a24f-2dd04ada6551"},"source":["# Best hyperparameter combination\n","models.best_params_"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'class_wgt': 'equi', 'encoding': 'amplit', 'n_copies': 2, 'rescale': 1}"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"ad4sBWfxfESC","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}