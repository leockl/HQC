{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "jNP6zCjXZwl8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "SxgWfc2GZwmR"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "class HQC_gpu(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"The Helstrom Quantum Centroid (HQC) classifier is a quantum-inspired supervised \n",
    "    classification approach for data with binary classes (ie. data with 2 classes only).\n",
    "                         \n",
    "    Parameters\n",
    "    ----------\n",
    "    rescale : int or float, default = 1\n",
    "        The dataset rescaling factor. A parameter used for rescaling the dataset. \n",
    "    encoding : str, default = 'amplit'\n",
    "        The encoding method used to encode vectors into quantum densities. Possible values:\n",
    "        'amplit', 'stereo'. 'amplit' means using the amplitude encoding method. 'stereo' means \n",
    "        using the inverse of the standard stereographic projection encoding method. Default set \n",
    "        to 'amplit'.\n",
    "    n_copies : int, default = 1\n",
    "        The number of copies to take for each quantum density. This is equivalent to taking \n",
    "        the n-fold Kronecker tensor product for each quantum density.\n",
    "    class_wgt : str, default = 'equi'\n",
    "        The class weights assigned to the Quantum Helstrom observable terms. Possible values: \n",
    "        'equi', 'weighted'. 'equi' means assigning equal weights of 1/2 (equiprobable) to the\n",
    "        two classes in the Quantum Helstrom observable. 'weighted' means assigning weights equal \n",
    "        to the proportion of the number of rows in each class to the two classes in the Quantum \n",
    "        Helstrom observable. Default set to 'equi'.\n",
    "    n_splits : int, default = 1\n",
    "        The number of subset splits performed on the input dataset row-wise and on the number \n",
    "        of eigenvalues/eigenvectors of the Quantum Helstrom observable for optimal speed \n",
    "        performance. If 1 is given, no splits are performed. For optimal speed, recommend \n",
    "        using small values as close to 1 as possible. If memory blow-out occurs, increase \n",
    "        n_splits.\n",
    "    dtype : torch.float32 or torch.float64, default = torch.float64\n",
    "        The float datatype used for the elements in the Pytorch tensor dataset. Datatype has to\n",
    "        be of float to ensure calculations are done in float rather than integer. To achieve\n",
    "        higher n_copies without memory blow-out issues, reduce float precision, which may or may   \n",
    "        not affect accuracy.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : ndarray, shape (2,)\n",
    "        Sorted binary classes.\n",
    "    centroids_ : tensor, size (2, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Quantum Centroids for class with index 0 and 1 respectively. Stored in GPU.\n",
    "    hels_obs_ : tensor, size ((n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Quantum Helstrom observable. Stored in GPU.\n",
    "    proj_sums_ : tensor, size (2, (n_features + 1)**n_copies, (n_features + 1)**n_copies)\n",
    "        Sum of the projectors of the Quantum Helstrom observable's eigenvectors, which has\n",
    "        corresponding positive and negative eigenvalues respectively. Stored in GPU.\n",
    "    hels_bound_ : float\n",
    "        Helstrom bound is the upper bound of the probability that one can correctly \n",
    "        discriminate whether a quantum density is of which of the two binary quantum density \n",
    "        pattern. Stored in CPU.         \n",
    "    \"\"\"\n",
    "    # Added binary_only tag as required by sklearn check_estimator\n",
    "    def _more_tags(self):\n",
    "        return {'binary_only': True}        \n",
    "    \n",
    "    \n",
    "    # Initialize model hyperparameters\n",
    "    def __init__(self, \n",
    "                 rescale = 1,\n",
    "                 encoding = 'amplit',\n",
    "                 n_copies = 1,                   \n",
    "                 class_wgt = 'equi', \n",
    "                 n_splits = 1,\n",
    "                 dtype = torch.float64):\n",
    "        self.rescale = rescale\n",
    "        self.encoding = encoding\n",
    "        self.n_copies = n_copies\n",
    "        self.class_wgt = class_wgt\n",
    "        self.n_splits = n_splits\n",
    "        self.dtype = dtype\n",
    "        \n",
    "        # Raise error if dtype is not torch.float32 or torch.float64\n",
    "        if self.dtype not in [torch.float32, torch.float64]:\n",
    "            raise ValueError('dtype should be torch.float32 or torch.float64 only')\n",
    "        \n",
    "    \n",
    "    # Function for kronecker tensor product of PyTorch tensors, set as global function\n",
    "    global kronecker\n",
    "    def kronecker(A, B):\n",
    "        return torch.einsum('nab,ncd->nacbd', A, B).view(A.size(0), \n",
    "                                                         A.size(1)*B.size(1), \n",
    "                                                         A.size(2)*B.size(2))\n",
    "    \n",
    "    \n",
    "    # Function for fit\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Perform HQC classification with the inverse of the standard stereographic \n",
    "        projection encoding, with the option to rescale the dataset prior to encoding.\n",
    "                \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The training input samples. An array of int or float.\n",
    "        y : array-like, shape (n_samples,)\n",
    "            The training input binary target values. An array of str, int or float.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        # Check that arrays X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        # Ensure target array y is of non-regression type  \n",
    "        # Added as required by sklearn check_estimator\n",
    "        check_classification_targets(y)\n",
    "            \n",
    "        # Store binary classes and encode y into binary class indexes 0 and 1\n",
    "        self.classes_, y_class_index = np.unique(y, return_inverse = True)\n",
    "\n",
    "        # Raise error if there are more than 2 classes\n",
    "        if len(self.classes_) > 2:  \n",
    "            raise ValueError('only 2 classes are supported')\n",
    "        \n",
    "        # Cast array X into a floating point tensor to ensure all following calculations below  \n",
    "        # are done in float rather than integer, and send tensor X from CPU to GPU\n",
    "        X = torch.tensor(X, dtype = self.dtype).cuda()\n",
    "        \n",
    "        # Rescale X\n",
    "        X = self.rescale*X\n",
    "        \n",
    "        # Calculate sum of squares of each row (sample) in X\n",
    "        X_sq_sum = (X**2).sum(dim = 1)\n",
    "        \n",
    "        # Number of rows in X\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Number of columns in X\n",
    "        n = X.shape[1]\n",
    "        \n",
    "        # Calculate X' using amplitude or inverse of the standard stereographic projection \n",
    "        # encoding method\n",
    "        if self.encoding == 'amplit':\n",
    "            X_prime = normalize(torch.cat([X, torch.ones(m, dtype = self.dtype) \\\n",
    "                                           .reshape(-1, 1).cuda()], dim = 1), p = 2, dim = 1)\n",
    "        elif self.encoding == 'stereo':\n",
    "            X_prime = (1 / (X_sq_sum + 1)).reshape(-1, 1)*(torch.cat((2*X, (X_sq_sum - 1) \\\n",
    "                                                                      .reshape(-1, 1)), dim = 1))\n",
    "        else:\n",
    "            raise ValueError('encoding should be \"amplit\" or \"stereo\"')\n",
    "        \n",
    "        # Number of columns in X', set as global variable\n",
    "        global n_prime\n",
    "        n_prime = n + 1\n",
    "        \n",
    "        # Function to calculate terms in the Quantum Centroids and quantum Helstrom \n",
    "        # observable for each class, per subset split\n",
    "        def centroids_terms_func(i):\n",
    "            # Cast array y_class_index into a tensor and send from CPU to GPU\n",
    "            # Determine rows (samples) in X' belonging to either class\n",
    "            X_prime_class = X_prime[torch.CharTensor(y_class_index).cuda() == i]\n",
    "                                    \n",
    "            # Split X' belonging to either class into n_splits subsets, row-wise\n",
    "            # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
    "            # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
    "            X_prime_class_split_arr = np.array_split(X_prime_class.cpu().numpy(),\n",
    "                                                     indices_or_sections = self.n_splits,\n",
    "                                                     axis = 0)\n",
    "            \n",
    "            # Cast arrays back to tensors and send back from CPU to GPU\n",
    "            X_prime_class_split = [torch.tensor(a, dtype = self.dtype).cuda() \n",
    "                                   for a in X_prime_class_split_arr]\n",
    "            \n",
    "            # Function to calculate sum of quantum densities belonging to each class, \n",
    "            # per subset split\n",
    "            def X_prime_class_split_func(j):\n",
    "                # Counter for j-th split of X'\n",
    "                X_prime_class_split_jth = X_prime_class_split[j]\n",
    "                \n",
    "                # Number of rows (samples) in j-th split of X'\n",
    "                m_class_split = X_prime_class_split_jth.shape[0]\n",
    "                \n",
    "                # Encode vectors into quantum densities\n",
    "                density_chunk = torch.matmul(X_prime_class_split_jth.view(m_class_split, \n",
    "                                                                          n_prime, 1),\n",
    "                                             X_prime_class_split_jth.view(m_class_split, \n",
    "                                                                          1, n_prime))\n",
    "                \n",
    "                # Calculate n-fold Kronecker tensor product\n",
    "                if self.n_copies == 1:\n",
    "                    density_chunk = density_chunk\n",
    "                else:\n",
    "                    density_chunk_copy = density_chunk\n",
    "                    for b in range(self.n_copies - 1):\n",
    "                        density_chunk = kronecker(density_chunk, density_chunk_copy)\n",
    "                    \n",
    "                # Calculate sum of quantum densities\n",
    "                density_chunk_sum = density_chunk.sum(dim = 0)\n",
    "                return density_chunk_sum\n",
    "\n",
    "            # Number of rows/columns in density matrix, set as global variable\n",
    "            global density_nrow_ncol\n",
    "            density_nrow_ncol = n_prime**self.n_copies\n",
    "            \n",
    "            # Initialize array density_class_sum\n",
    "            density_class_sum = torch.zeros([density_nrow_ncol, density_nrow_ncol], \n",
    "                                            dtype = self.dtype).cuda()\n",
    "            for c in range(self.n_splits):\n",
    "                # Calculate sum of quantum densities belonging to either class\n",
    "                density_class_sum = density_class_sum + X_prime_class_split_func(c)\n",
    "            \n",
    "            # Number of rows (samples) in X' belonging to either class\n",
    "            m_class = X_prime_class.shape[0]\n",
    "            \n",
    "            # Function to calculate centroid belonging to either class\n",
    "            def centroid():\n",
    "                # Calculate Quantum Centroid belonging to either class\n",
    "                # Added ZeroDivisionError as required by sklearn check_estimator\n",
    "                try:\n",
    "                    centroid = (1 / m_class)*density_class_sum\n",
    "                except ZeroDivisionError:\n",
    "                    centroid = 0 \n",
    "                return centroid\n",
    "            \n",
    "            # Calculate centroid belonging to either class\n",
    "            centroid_class = centroid()\n",
    "            \n",
    "            # Calculate terms in the quantum Helstrom observable belonging to either class\n",
    "            if self.class_wgt == 'equi':\n",
    "                hels_obs_terms = 0.5*centroid_class\n",
    "            elif self.class_wgt == 'weighted':\n",
    "                hels_obs_terms = (m_class / m)*centroid_class\n",
    "            else:\n",
    "                raise ValueError('class_wgt should be \"equi\" or \"weighted\"')\n",
    "            return m_class, centroid_class, hels_obs_terms\n",
    "        \n",
    "        # Calculate Quantum Centroids and terms in the quantum Helstrom observable belonging \n",
    "        # to either class\n",
    "        centroids_terms = [centroids_terms_func(0), centroids_terms_func(1)] \n",
    "                    \n",
    "        # Determine Quantum Centroids\n",
    "        self.centroids_ = torch.stack([centroids_terms[0][1], centroids_terms[1][1]], dim = 0)\n",
    "                \n",
    "        # Calculate quantum Helstrom observable\n",
    "        self.hels_obs_ = centroids_terms[0][2] - centroids_terms[1][2] \n",
    "                \n",
    "        # Calculate eigenvalues w and eigenvectors v of the quantum Helstrom observable\n",
    "        w, v = torch.symeig(self.hels_obs_, eigenvectors = True)\n",
    "          \n",
    "        # Length of w\n",
    "        len_w = len(w)\n",
    "        \n",
    "        # Initialize array eigval_class\n",
    "        eigval_class = torch.empty_like(w, dtype = self.dtype).cuda()\n",
    "        for d in range(len_w):\n",
    "            # Create an array of 0s and 1s to indicate positive and negative eigenvalues\n",
    "            # respectively\n",
    "            if w[d] > 0:\n",
    "                eigval_class[d] = 0\n",
    "            else:\n",
    "                eigval_class[d] = 1\n",
    "        \n",
    "        # Transpose matrix v containing eigenvectors to row-wise\n",
    "        eigvec = v.T\n",
    "        \n",
    "        # Function to calculate sum of the projectors corresponding to positive and negative\n",
    "        # eigenvalues respectively\n",
    "        def sum_proj_func(e):\n",
    "            # Split eigenvectors belonging to positive or negative eigenvalues into n_splits subsets\n",
    "            # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
    "            # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
    "            eigvec_class_split_arr_full = np.array_split(eigvec.cpu().numpy()[eigval_class.cpu() == e],\n",
    "                                                         indices_or_sections = self.n_splits,\n",
    "                                                         axis = 0)\n",
    "            \n",
    "            # Remove empty rows in eigvec_class_split_arr_full\n",
    "            eigvec_class_split_arr = [f for f in eigvec_class_split_arr_full if f.shape[0] > 0]\n",
    "\n",
    "            # Cast arrays back to tensors and send back from CPU to GPU\n",
    "            eigvec_class_split = [torch.tensor(g, dtype = self.dtype).cuda() \n",
    "                                  for g in eigvec_class_split_arr]             \n",
    "            \n",
    "            # Function to calculate sum of the projectors corresponding to positive and negative\n",
    "            # eigenvalues respectively, per subset split\n",
    "            def eigvec_class_split_func(h):\n",
    "                # Counter for h-th split of eigvec\n",
    "                eigvec_class_split_hth = eigvec_class_split[h]\n",
    "                \n",
    "                # Number of rows (samples) in h-th split of eigvec\n",
    "                m_eigvec_class_split = eigvec_class_split_hth.shape[0]\n",
    "                \n",
    "                # Calculate projectors corresponding to positive and negative eigenvalues  \n",
    "                # respectively, per subset split\n",
    "                proj_split = torch.matmul(eigvec_class_split_hth.view(m_eigvec_class_split, \n",
    "                                                                      density_nrow_ncol, 1),\n",
    "                                          eigvec_class_split_hth.view(m_eigvec_class_split, \n",
    "                                                                      1, density_nrow_ncol))\n",
    "                \n",
    "                # Calculate sum of projectors\n",
    "                proj_split_sum = proj_split.sum(dim = 0)\n",
    "                return proj_split_sum\n",
    "            \n",
    "            # Determine length of eigvec_class_split_arr\n",
    "            eigvec_class_split_arr_len = len(eigvec_class_split_arr)\n",
    "\n",
    "            # Initialize array proj_class_sum\n",
    "            proj_class_sum = torch.zeros([density_nrow_ncol, density_nrow_ncol], \n",
    "                                         dtype = self.dtype).cuda()  \n",
    "            for k in range(eigvec_class_split_arr_len):\n",
    "                # Calculate sum of the projectors corresponding to positive and negative eigenvalues\n",
    "                # respectively\n",
    "                proj_class_sum = proj_class_sum + eigvec_class_split_func(k)\n",
    "            return proj_class_sum\n",
    "        \n",
    "        # Calculate sum of the projectors corresponding to positive and negative eigenvalues \n",
    "        # respectively\n",
    "        self.proj_sums_ = torch.stack([sum_proj_func(0), sum_proj_func(1)], dim = 0)        \n",
    "                       \n",
    "        # Calculate Helstrom bound\n",
    "        self.hels_bound_ = (centroids_terms[0][0] / m)*torch.einsum('ij,ji->', self.centroids_[0], \n",
    "                                                                   self.proj_sums_[0]).item() \\\n",
    "                           + (centroids_terms[1][0] / m)*torch.einsum('ij,ji->', self.centroids_[1], \n",
    "                                                                     self.proj_sums_[1]).item()\n",
    "        return self\n",
    "        \n",
    "    \n",
    "    # Function for predict_proba\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Performs HQC classification on X and returns the trace of the dot product of the densities \n",
    "        and the sum of the projectors with corresponding positive and negative eigenvalues respectively.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples. An array of int or float.       \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        trace_matrix : tensor, size (n_samples, 2)\n",
    "            Column index 0 corresponds to the trace of the dot product of the densities and the sum  \n",
    "            of projectors with positive eigenvalues. Column index 1 corresponds to the trace of the  \n",
    "            dot product of the densities and the sum of projectors with negative eigenvalues. A tensor \n",
    "            of float. Stored in GPU.\n",
    "        \"\"\"\n",
    "        # Send tensor self.proj_sums_ from GPU to CPU and cast into an array\n",
    "        self.proj_sums_arr_ = self.proj_sums_.cpu().numpy()\n",
    "                \n",
    "        # Check if fit had been called\n",
    "        check_is_fitted(self, ['proj_sums_arr_'])\n",
    "               \n",
    "        # Input validation of array X\n",
    "        X = check_array(X)\n",
    "                 \n",
    "        # Cast array X into a floating point tensor to ensure all following calculations below  \n",
    "        # are done in float rather than integer, and send tensor X from CPU to GPU\n",
    "        X = torch.tensor(X, dtype = self.dtype).cuda()\n",
    "        \n",
    "        # Rescale X\n",
    "        X = self.rescale*X        \n",
    "        \n",
    "        # Calculate sum of squares of each row (sample) in X\n",
    "        X_sq_sum = (X**2).sum(dim = 1)\n",
    "        \n",
    "        # Number of rows in X\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Number of columns in X\n",
    "        n = X.shape[1]\n",
    "\n",
    "        # Calculate X' using amplitude or inverse of the standard stereographic projection \n",
    "        # encoding method\n",
    "        if self.encoding == 'amplit':\n",
    "            X_prime = normalize(torch.cat([X, torch.ones(m, dtype = self.dtype) \\\n",
    "                                           .reshape(-1, 1).cuda()], dim = 1), p = 2, dim = 1)\n",
    "        elif self.encoding == 'stereo':\n",
    "            X_prime = (1 / (X_sq_sum + 1)).reshape(-1, 1)*(torch.cat((2*X, (X_sq_sum - 1) \\\n",
    "                                                                      .reshape(-1, 1)), dim = 1))\n",
    "        else:\n",
    "            raise ValueError('encoding should be \"amplit\" or \"stereo\"')\n",
    "                       \n",
    "        # Function to calculate trace values for each class\n",
    "        def trace_func(i):\n",
    "            # Split X' into n_splits subsets, row-wise\n",
    "            # Send tensors from GPU to CPU and cast tensors into arrays, use np.array_split()\n",
    "            # because the equivalent torch.chunk() doesn't behave similarly to np.array_split()\n",
    "            X_prime_split_arr_full = np.array_split(X_prime.cpu().numpy(),\n",
    "                                                    indices_or_sections = self.n_splits,\n",
    "                                                    axis = 0)\n",
    "            \n",
    "            # Remove empty rows in X_prime_split_arr_full\n",
    "            X_prime_split_arr = [a for a in X_prime_split_arr_full if a.shape[0] > 0]\n",
    "\n",
    "            # Cast arrays back to tensors and send back from CPU to GPU\n",
    "            X_prime_split = [torch.tensor(q, dtype = self.dtype).cuda() for q in X_prime_split_arr]\n",
    "            \n",
    "            # Function to calculate trace values for each class, per subset split\n",
    "            def trace_split_func(j):\n",
    "                # Counter for j-th split X'\n",
    "                X_prime_split_jth = X_prime_split[j]\n",
    "                \n",
    "                # Number of rows (samples) in j-th split X'\n",
    "                X_prime_split_m = X_prime_split_jth.shape[0]\n",
    "                \n",
    "                # Encode vectors into quantum densities\n",
    "                density_chunk = torch.matmul(X_prime_split_jth.view(X_prime_split_m, n_prime, 1),\n",
    "                                             X_prime_split_jth.view(X_prime_split_m, 1, n_prime))\n",
    "                \n",
    "                # Calculate n-fold Kronecker tensor product\n",
    "                if self.n_copies == 1:\n",
    "                    density_chunk = density_chunk\n",
    "                else:\n",
    "                    density_chunk_copy = density_chunk\n",
    "                    for b in range(self.n_copies - 1):\n",
    "                        density_chunk = kronecker(density_chunk, density_chunk_copy)\n",
    "                        \n",
    "                # Calculate trace of the dot product of density of each row and sum of projectors\n",
    "                # with corresponding positive and negative eigenvalues respectively\n",
    "                return torch.einsum('bij,ji->b', density_chunk, self.proj_sums_[i])\n",
    "            \n",
    "            # Determine length of X_prime_split_arr\n",
    "            X_prime_split_arr_len = len(X_prime_split_arr)\n",
    "\n",
    "            # Initialize array trace_class\n",
    "            trace_class = torch.empty([0], dtype = self.dtype).cuda()\n",
    "            for c in range(X_prime_split_arr_len):\n",
    "                # Calculate trace values for each class, per subset split\n",
    "                trace_class = torch.cat([trace_class, trace_split_func(c)], dim = 0)\n",
    "            return trace_class\n",
    "        \n",
    "        # Calculate trace values for each class\n",
    "        trace_matrix = torch.stack([trace_func(0), trace_func(1)], dim = 1).cpu().numpy()\n",
    "        return trace_matrix\n",
    "                \n",
    "    \n",
    "    # Function for predict\n",
    "    def predict(self, X):\n",
    "        \"\"\"Performs HQC classification on X and returns the binary classes.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input samples. An array of int or float.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        self.classes_[predict_trace_index] : array-like, shape (n_samples,)\n",
    "            The predicted binary classes. An array of str, int or float.\n",
    "        \"\"\"\n",
    "        # Determine column index with the higher trace value in trace_matrix\n",
    "        # If both columns have the same trace value, returns column index 1, which is different \n",
    "        # to np.argmax() which returns column index 0\n",
    "        predict_trace_index = torch.argmax(torch.tensor(self.predict_proba(X), dtype = self.dtype).cuda(), axis = 1)\n",
    "        # Returns the predicted binary classes\n",
    "        return self.classes_[predict_trace_index.cpu().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "gyVek3DUZwmc"
   },
   "outputs": [],
   "source": [
    "# Read in dataset\n",
    "df = pd.read_csv('sample_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "PEm2B8-vZwmp",
    "outputId": "830bdfcf-af20-4984-a691-b2a681524dfb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Hemoglobin</th>\n",
       "      <th>Absolute Lymphocyte Count</th>\n",
       "      <th>Absolute Neutrophil Count</th>\n",
       "      <th>Platelet Count</th>\n",
       "      <th>C-Reactive Protein</th>\n",
       "      <th>Ferritin</th>\n",
       "      <th>D-DIMER</th>\n",
       "      <th>Absolute Basophil Count</th>\n",
       "      <th>Absolute Eosinophil Count</th>\n",
       "      <th>Absolute Monocyte Count</th>\n",
       "      <th>Lactate Dehydrogenase</th>\n",
       "      <th>Red Blood Cell Count</th>\n",
       "      <th>Lymp/Neut</th>\n",
       "      <th>Age</th>\n",
       "      <th>positive</th>\n",
       "      <th>Male</th>\n",
       "      <th>TestResult</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Race</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.27</td>\n",
       "      <td>170.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.36</td>\n",
       "      <td>0.433942</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1.75</td>\n",
       "      <td>10.18</td>\n",
       "      <td>415.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.14</td>\n",
       "      <td>0.119211</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>6.8</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.80</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.63</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.34</td>\n",
       "      <td>0.062818</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15.1</td>\n",
       "      <td>1.16</td>\n",
       "      <td>11.01</td>\n",
       "      <td>262.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.61</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "      <td>Female</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>11.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.12</td>\n",
       "      <td>111.0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.50</td>\n",
       "      <td>340.0</td>\n",
       "      <td>4.39</td>\n",
       "      <td>0.053373</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>Male</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Hemoglobin  Absolute Lymphocyte Count  ...  TestResult     Sex   Race\n",
       "0           0         7.9                        NaN  ...    negative    Male  Black\n",
       "1           1        13.3                       1.75  ...    negative  Female  Black\n",
       "2           2         6.8                       0.45  ...    negative    Male  Black\n",
       "3           3        15.1                       1.16  ...    negative  Female  White\n",
       "4           4        11.1                        NaN  ...    negative    Male  White\n",
       "\n",
       "[5 rows x 20 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RcxyNtWZwm8",
    "outputId": "4e094ced-d5e8-4660-997c-018ac3a4dabe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1072, 20)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check no. of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgnxWSI9ZwnI"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hX8ruslSZwnL",
    "outputId": "906a2744-6ddf-43b6-f211-bd3656813f10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0                     int64\n",
       "Hemoglobin                   float64\n",
       "Absolute Lymphocyte Count    float64\n",
       "Absolute Neutrophil Count    float64\n",
       "Platelet Count               float64\n",
       "C-Reactive Protein           float64\n",
       "Ferritin                     float64\n",
       "D-DIMER                      float64\n",
       "Absolute Basophil Count      float64\n",
       "Absolute Eosinophil Count    float64\n",
       "Absolute Monocyte Count      float64\n",
       "Lactate Dehydrogenase        float64\n",
       "Red Blood Cell Count         float64\n",
       "Lymp/Neut                    float64\n",
       "Age                            int64\n",
       "positive                       int64\n",
       "Male                           int64\n",
       "TestResult                    object\n",
       "Sex                           object\n",
       "Race                          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check datatypes\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "VPO3NToHZwnV"
   },
   "outputs": [],
   "source": [
    "# Cast \"Age\" feature to float datatype for floating point calculations later on\n",
    "df['Age'] = df['Age'].astype(float)\n",
    "\n",
    "# Cast \"Male\" feature to object datatype as these are categorical features\n",
    "df['Male'] = df['Male'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fMlc4gibZwnn",
    "outputId": "d938292f-d0d6-4fb5-ac26-3a764c1929f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D-DIMER                      0.566231\n",
       "Lactate Dehydrogenase        0.491604\n",
       "Ferritin                     0.474813\n",
       "C-Reactive Protein           0.333955\n",
       "Absolute Lymphocyte Count    0.222015\n",
       "Lymp/Neut                    0.222015\n",
       "Race                         0.159515\n",
       "TestResult                   0.159515\n",
       "Sex                          0.159515\n",
       "Absolute Basophil Count      0.138060\n",
       "Absolute Neutrophil Count    0.138060\n",
       "Absolute Eosinophil Count    0.138060\n",
       "Absolute Monocyte Count      0.138060\n",
       "Platelet Count               0.014925\n",
       "Hemoglobin                   0.013060\n",
       "Red Blood Cell Count         0.012127\n",
       "Age                          0.000000\n",
       "positive                     0.000000\n",
       "Male                         0.000000\n",
       "Unnamed: 0                   0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 48,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values\n",
    "(df.isnull().sum(axis=0)/df.shape[0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJ_r_I9xZwn0",
    "outputId": "4e0dbd2e-9080-4d2a-a451-3dfcd8327a2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(457, 20)"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with no complete blood counts\n",
    "df_drop_count = df.dropna(axis=0, how='any', subset=[feature_name for feature_name in df.columns if feature_name[-5:] in ['Count']])\n",
    "df_drop_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sme7Ila-Zwn_",
    "outputId": "8c391969-4380-41c9-b48b-816550d364e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 20)"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop rows with no imflammatory markers at all\n",
    "df_drop_count_marker = df_drop_count.dropna(axis=0, how='all', subset=['C-Reactive Protein', 'Ferritin', 'Lactate Dehydrogenase'])\n",
    "df_drop_count_marker.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yBBYH0SmZwoM"
   },
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ph5Zj5V0ZwoP",
    "outputId": "13a3a456-89da-4307-8b9d-4459e049e575"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(414, 15)"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop features not used in the model from the paper\n",
    "df_drop_count_marker_feat = df_drop_count_marker.drop(['Unnamed: 0', 'D-DIMER', 'TestResult', 'Sex', 'Race'], axis=1)\n",
    "df_drop_count_marker_feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y63btWnlZwob"
   },
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "I5nwnAHCZwod"
   },
   "outputs": [],
   "source": [
    "# Extract features\n",
    "df_X = df_drop_count_marker_feat.drop(['positive'], axis=1)\n",
    "\n",
    "# Extract target variable\n",
    "df_y = df_drop_count_marker_feat[['positive']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "T7Lvx18QZwot"
   },
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "##### NO STRATIFIED SAMPLING IN TRAIN/TEST SPLIT #####\n",
    "df_X_train, df_X_test, df_y_train, df_y_test = model_selection.train_test_split(df_X, df_y, test_size=0.3, random_state=0)\n",
    "\n",
    "##### WITH STRATIFIED SAMPLING IN TRAIN/TEST SPLIT #####\n",
    "# df_X_train, df_X_test, df_y_train, df_y_test = model_selection.train_test_split(df_X, df_y, test_size=0.3, random_state=0, stratify=df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "RXduiwI7Zwo3"
   },
   "outputs": [],
   "source": [
    "# Extract continuous features from training set\n",
    "df_X_train_con = df_X_train.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Calculate mean and std dev of continuous features in training set\n",
    "transformer = preprocessing.StandardScaler().fit(df_X_train_con.values)\n",
    "\n",
    "# Normalize continuous features in training set\n",
    "X_train_con_norm = transformer.transform(df_X_train_con.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "s5dToouQZwpE"
   },
   "outputs": [],
   "source": [
    "# Extract continuous features from test set\n",
    "df_X_test_con = df_X_test.select_dtypes(exclude=['object'])\n",
    "\n",
    "# Normalize continuous features in test set (according to mean and std dev of continuous features in training set)\n",
    "X_test_con_norm = transformer.transform(df_X_test_con.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "jNOIeJVxZwpO"
   },
   "outputs": [],
   "source": [
    "# Extract categorical features from training set, ie. just the \"Male\" feature\n",
    "df_X_train_cat = df_X_train.select_dtypes(include=['object'])\n",
    "\n",
    "# Perform one-hot encoding on categorical features in training set\n",
    "df_X_train_cat_onehot = pd.get_dummies(df_X_train_cat)\n",
    "\n",
    "# Concatenate normalized continuous features and one-hot encoded categorical features for training set\n",
    "X_train_fe = np.concatenate([X_train_con_norm, df_X_train_cat_onehot.values], axis=1)\n",
    "\n",
    "# Imput missing values with zeros in training set\n",
    "X_train_fe_zero = np.nan_to_num(X_train_fe, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "s7NzGXNZZwpX"
   },
   "outputs": [],
   "source": [
    "# Extract categorical features from test set\n",
    "df_X_test_cat = df_X_test.select_dtypes(include=['object'])\n",
    "\n",
    "# Perform one-hot encoding on categorical features in test set\n",
    "df_X_test_cat_onehot = pd.get_dummies(df_X_test_cat)\n",
    "\n",
    "# Concatenate normalized continuous features and one-hot encoded categorical features for test set\n",
    "X_test_fe = np.concatenate([X_test_con_norm, df_X_test_cat_onehot.values], axis=1)\n",
    "\n",
    "# Imput missing values with zeros in test set\n",
    "X_test_fe_zero = np.nan_to_num(X_test_fe, nan=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fkh-XRySZwpj",
    "outputId": "0a98d9ac-90d3-4819-b685-3d746f59ed4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((289, 15), (125, 15), (289, 1), (125, 1))"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check no. of rows and columns \n",
    "X_train_fe_zero.shape, X_test_fe_zero.shape, df_y_train.shape, df_y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vs5Zimxnh_kh",
    "outputId": "5cd0ba7e-6a50-488a-80a6-168df2dbd5dd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14    0.0\n",
       "13    0.0\n",
       "12    0.0\n",
       "11    0.0\n",
       "10    0.0\n",
       "9     0.0\n",
       "8     0.0\n",
       "7     0.0\n",
       "6     0.0\n",
       "5     0.0\n",
       "4     0.0\n",
       "3     0.0\n",
       "2     0.0\n",
       "1     0.0\n",
       "0     0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values in X_train_fe_zero\n",
    "(pd.DataFrame(X_train_fe_zero).isnull().sum(axis=0)/pd.DataFrame(X_train_fe_zero).shape[0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jKtdMj5Ph_wT",
    "outputId": "13066f5b-f909-43fd-9357-d39659e3c380"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14    0.0\n",
       "13    0.0\n",
       "12    0.0\n",
       "11    0.0\n",
       "10    0.0\n",
       "9     0.0\n",
       "8     0.0\n",
       "7     0.0\n",
       "6     0.0\n",
       "5     0.0\n",
       "4     0.0\n",
       "3     0.0\n",
       "2     0.0\n",
       "1     0.0\n",
       "0     0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values in X_test_fe_zero\n",
    "(pd.DataFrame(X_test_fe_zero).isnull().sum(axis=0)/pd.DataFrame(X_test_fe_zero).shape[0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOLkv6mXh_2m",
    "outputId": "5fac147d-a34d-414f-99fa-004854759268"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values in df_y_train\n",
    "(df_y_train.isnull().sum(axis=0)/df_y_train.shape[0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxKgHvJfh_6e",
    "outputId": "17cba63c-36c7-4d42-d56b-c0834c0363b0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 62,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check missing values in df_y_test\n",
    "(df_y_test.isnull().sum(axis=0)/df_y_test.shape[0]).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKK75eo6iHXA",
    "outputId": "fd8f9007-ad1b-46e4-dbe0-bc6eb40d7a69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    74.394464\n",
       "1    25.605536\n",
       "Name: positive, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if class imbalance\n",
    "df_y_train['positive'].value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qsywlYWZwpx"
   },
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRggSUpwbHJ7"
   },
   "source": [
    "##### NO STRATIFIED SAMPLING IN TRAIN/TEST SPLIT #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UgqN4aLRZwpz"
   },
   "outputs": [],
   "source": [
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "# Using scikit-learn's GridSearchCV (with 7-folds following the paper)\n",
    "# Did not try n_copies=4 because it took too much memory or too much time\n",
    "param_grid = {'rescale':rescale_list1, 'encoding':['amplit', 'stereo'], 'n_copies':[1, 2, 3], 'class_wgt':['equi', 'weighted']}\n",
    "models = model_selection.GridSearchCV(HQC_gpu(n_splits=100, dtype=torch.float64), param_grid, scoring='roc_auc', cv=7).fit(X_train_fe_zero, df_y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-ZL1GZbZwp8",
    "outputId": "44b86f8c-1ba8-486d-88d9-02b6d80e3941"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4566864295125165"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best AUROC score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "heRdOWKFZwqG",
    "outputId": "1972fa6d-dc34-459a-80c2-79aee6de2269"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_wgt': 'equi', 'encoding': 'amplit', 'n_copies': 1, 'rescale': 1.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YXnfmknZwqP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5NwvPRR0cLJp",
    "outputId": "a97d58cc-3926-4231-dc13-faae0b349dd8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5345849802371541"
      ]
     },
     "execution_count": 64,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model (following the paper's hyperparameter selection)\n",
    "rand_forest_model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features='sqrt', random_state=0).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = rand_forest_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sYH8IEtcLNj",
    "outputId": "0edb9613-9aa3-4601-d823-a9453f79368b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5345849802371541"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression model (following the paper's hyperparameter selection)\n",
    "log_reg_model = LogisticRegression().fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = rand_forest_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aHMvn6itcLT1",
    "outputId": "63c3b5b9-c270-41c7-b41e-673b7e70a406"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM model (following the paper's hyperparameter selection)\n",
    "svm_model = svm.SVC(probability=True).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = svm_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWmNgty5cL1V",
    "outputId": "0772d8c5-c95c-45b7-fb13-2e80829c243d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4813899868247694"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nueral net model (following the paper's hyperparameter selection, with the exception of added max_iter as model was not converging)\n",
    "mlp_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=0, max_iter=250).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = mlp_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XO_zRrvLcLyG",
    "outputId": "0371bc1b-c1a6-46b6-f0d6-e181209dc2e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4744729907773386"
      ]
     },
     "execution_count": 68,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD model (following the paper's hyperparameter selection)\n",
    "sgd_model = SGDClassifier(loss=\"modified_huber\", penalty=\"l2\", max_iter=500, random_state=0).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = sgd_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R5xmKNaBcLwn",
    "outputId": "266fcd9a-e669-4f90-f97e-ccff4c175b2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5171277997364954"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost model (following the paper's hyperparameter selection)\n",
    "xgboost_model = XGBClassifier(random_state=0).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = xgboost_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZ79DKAAcLtL",
    "outputId": "92804194-f9d2-45c8-ef3e-6726fb34ab69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5699934123847167"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADABoost model (following the paper's hyperparameter selection)\n",
    "ada_model = AdaBoostClassifier(n_estimators=100, random_state=0).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = ada_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WHB7YtQZbS7b"
   },
   "source": [
    "##### WITH STRATIFIED SAMPLING IN TRAIN/TEST SPLIT #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "cFHrj_sdJJe9"
   },
   "outputs": [],
   "source": [
    "# Create rescale hyperparamter list [0.1, 0.5, 1, 1.5,...,10.0]\n",
    "rescale_list1 = [0.1]\n",
    "rescale_list2 = np.linspace(0.5, 10, 20).tolist()\n",
    "rescale_list1.extend(rescale_list2)\n",
    "\n",
    "# Using scikit-learn's GridSearchCV (with 7-folds following the paper)\n",
    "# Did not try n_copies=4 because it took too much memory or too much time\n",
    "param_grid = {'rescale':rescale_list1, 'encoding':['amplit', 'stereo'], 'n_copies':[1, 2, 3], 'class_wgt':['equi', 'weighted']}\n",
    "models = model_selection.GridSearchCV(HQC_gpu(n_splits=100, dtype=torch.float64), param_grid, scoring='roc_auc', cv=7).fit(X_train_fe_zero, df_y_train.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EfYbn8t6JJkd",
    "outputId": "230bf3b5-b244-4eb9-e0c2-95d5ee4a7d21"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5267137096774194"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best AUROC score\n",
    "best_model = models.best_estimator_\n",
    "y_hat = best_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KDtfcSgWJPJC",
    "outputId": "029f6aa2-f218-407f-9680-ff28ab0063a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'class_wgt': 'equi', 'encoding': 'stereo', 'n_copies': 1, 'rescale': 8.0}"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best hyperparameter combination\n",
    "models.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lq7uovNJbw2q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rdJPOOOkbw95",
    "outputId": "5b1e2890-4d3e-4046-bb2c-8bbc08b59a0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5102486559139785"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random forest model (following the paper's hyperparameter selection)\n",
    "rand_forest_model = RandomForestClassifier(n_estimators=100, bootstrap=True, max_features='sqrt', random_state=0).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = rand_forest_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v19K-IWQbxAu",
    "outputId": "311c7f54-0993-47c9-fe47-b6d65294842e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5102486559139785"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic regression model (following the paper's hyperparameter selection)\n",
    "log_reg_model = LogisticRegression().fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = rand_forest_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liw1TlLvbxE1",
    "outputId": "a96f3c46-7da4-4c0a-c096-9129bdd79203"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM model (following the paper's hyperparameter selection)\n",
    "svm_model = svm.SVC(probability=True).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = svm_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Utkfr_-hbxQT",
    "outputId": "cb7289db-eb51-47c5-d96e-e05ff7927bb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4554771505376344"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nueral net model (following the paper's hyperparameter selection)\n",
    "mlp_model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=0).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = mlp_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ascNqtEmbxVz",
    "outputId": "b28c4b12-a8c8-45d4-e9e5-65f501e10264"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.46135752688172044"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SGD model (following the paper's hyperparameter selection)\n",
    "sgd_model = SGDClassifier(loss=\"modified_huber\", penalty=\"l2\", max_iter=500, random_state=0).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = sgd_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "th2JCgGTbxZp",
    "outputId": "fac30bd0-3b92-469a-8749-ed43030186d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.545866935483871"
      ]
     },
     "execution_count": 38,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost model (following the paper's hyperparameter selection)\n",
    "xgboost_model = XGBClassifier(random_state=0).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = xgboost_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9AZwrG22bxdf",
    "outputId": "777ff8e1-927f-4209-b723-ec056857d21d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5751008064516128"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ADABoost model (following the paper's hyperparameter selection)\n",
    "ada_model = AdaBoostClassifier(n_estimators=100, random_state=0).fit(X_train_fe_zero, df_y_train.values.ravel())\n",
    "y_hat = ada_model.predict(X_test_fe_zero)\n",
    "metrics.roc_auc_score(df_y_test.values.ravel(), y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g3PDuvSnJJnv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Covid_1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
